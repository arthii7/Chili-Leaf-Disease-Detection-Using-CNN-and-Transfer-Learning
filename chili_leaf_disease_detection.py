# -*- coding: utf-8 -*-
"""chili leaf disease detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wVZVqcNutyh70d_yYKUNZPCwyicjw_rk
"""

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing import image
from tensorflow.keras.optimizers import RMSprop
from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D,BatchNormalization, Input, concatenate
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.preprocessing.image import ImageDataGenerator

from tensorflow.keras import layers,models

IMAGE_SIZE=200
BATCH_SIZE=32
CHANNELS=3
EPOCHS=100

dataset=tf.keras.preprocessing.image_dataset_from_directory(
    "/content/drive/MyDrive/Chilli Aug- new",
    shuffle= True,
    image_size=(IMAGE_SIZE, IMAGE_SIZE),
    batch_size=BATCH_SIZE

)

dataset=tf.keras.preprocessing.image_dataset_from_directory(
    "/content/drive/MyDrive/Chilli dataset-new - Copy",
    shuffle= True,
    image_size=(IMAGE_SIZE, IMAGE_SIZE),
    batch_size=BATCH_SIZE

)

class_names=dataset.class_names
class_names

plt.figure(figsize=(10,10))
for image_batch, label_batch in dataset.take(1):
    print(image_batch.shape)
    for i in range(12):
        plt.subplot(3,4,i+1)
        plt.imshow(image_batch[i].numpy().astype("uint8"))
        plt.title(class_names[label_batch[i]])
        plt.axis('off')

def dataset_split(ds, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=1000):
    ds_size=len(ds)

    if shuffle:
        ds=ds.shuffle(shuffle_size, seed=12)
    train_size=int(train_split*ds_size)
    val_size=int(val_split*ds_size)

    train_ds=ds.take(train_size)

    val_ds=ds.skip(train_size).take(val_size)
    test_ds=ds.skip(train_size).skip(val_size)

    return train_ds,val_ds,test_ds

train_ds,val_ds, test_ds= dataset_split(dataset)

train_ds=train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)
test_ds=test_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)
val_ds=val_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)

resize_and_rescale = tf.keras.Sequential([
    layers.Resizing(IMAGE_SIZE, IMAGE_SIZE),
    layers.Rescaling(1.0/255)
])

data_augmentation=tf.keras.Sequential([
    layers.RandomFlip('horizontal_and_vertical'),
    layers.RandomRotation(0.2)
])

"""## **CNN**"""

input_shape=(BATCH_SIZE,IMAGE_SIZE,IMAGE_SIZE,CHANNELS)
n_classes=7
model=models.Sequential([
    resize_and_rescale,
    data_augmentation,
    layers.Conv2D(64,(3,3),activation='relu',input_shape=input_shape),
    layers.MaxPooling2D((2,2)),
    layers.Conv2D(64,kernel_size=(3,3),activation='relu'),
    layers.MaxPooling2D((2,2)),
    layers.Conv2D(64,(3,3),activation='relu'),
    layers.MaxPooling2D((2,2)),
    layers.Conv2D(64,(3,3),activation='relu'),
    layers.MaxPooling2D((2,2)),
    layers.Conv2D(64,(3,3),activation='relu'),
    layers.MaxPooling2D((2,2)),

    layers.Flatten(),
    layers.Dense(64,activation='relu'),
    layers.Dense(n_classes, activation='softmax')

])
model.build(input_shape=input_shape)

model.summary()

model.compile(
    optimizer='adam',
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
    metrics=['accuracy']
)

history_1=model.fit(
train_ds,
    epochs=40,
    batch_size=64,
    verbose=1,
    validation_data=val_ds
)

scores=model.evaluate(test_ds)
scores

plt.figure(figsize=(8,6))

plt.subplot(1,2,1)
plt.plot(history_1.history['accuracy'],label='accuarcy')
plt.plot(history_1.history['val_accuracy'],label='val_accuracy')
plt.legend()

plt.subplot(1,2,2)
plt.plot(history_1.history['loss'],label='loss')
plt.plot(history_1.history['val_loss'],label='val_loss')
plt.legend()

plt.show()

"""## **ALEXNET**"""

model_2 = models.Sequential()

#1st Layer
model_2.add(Conv2D(filters=96, input_shape=(200,200,3), kernel_size=(11,11), strides=(4,4), padding='same'))
model_2.add(tf.keras.layers.BatchNormalization())
model_2.add(Activation('relu'))
model_2.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))

#2nd Layer
model_2.add(Conv2D(filters=256, kernel_size=(5, 5), strides=(1,1), padding='same'))
model_2.add(tf.keras.layers.BatchNormalization())
model_2.add(Activation('relu'))
model_2.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))

#3rd Layer
model_2.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))
model_2.add(tf.keras.layers.BatchNormalization())
model_2.add(Activation('relu'))
#4th Layer
model_2.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))
model_2.add(tf.keras.layers.BatchNormalization())
model_2.add(Activation('relu'))

#5th Layer
model_2.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same'))
model_2.add(tf.keras.layers.BatchNormalization())
model_2.add(Activation('relu'))

model_2.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))

model_2.add(Flatten())
# 1st Fully Connected Layer
model_2.add(Dense(4096, input_shape=(200,200,3,)))
model_2.add(tf.keras.layers.BatchNormalization())
model_2.add(Activation('relu'))

model_2.add(Dropout(0.2))

#2nd Fully Connected Layer
model_2.add(Dense(4096))
model_2.add(tf.keras.layers.BatchNormalization())
model_2.add(Activation('relu'))

model_2.add(Dropout(0.2))

#3rd Fully Connected Layer
model_2.add(Dense(1000))
model_2.add(tf.keras.layers.BatchNormalization())
model_2.add(Activation('relu'))

model_2.add(Dropout(0.2))


#Output Layer
model_2.add(Dense(7))
model_2.add(tf.keras.layers.BatchNormalization())
model_2.add(Activation('softmax'))

#Model Summary
model_2.summary()

model_2.compile(optimizer='adam',
              loss='SparseCategoricalCrossentropy',
              metrics=['accuracy'])

epochs=50
history_2 = model_2.fit(
  train_ds,
  validation_data= val_ds,
 epochs=epochs
)

scores=model_2.evaluate(test_ds)
scores

plt.figure(figsize=(8,6))

plt.subplot(1,2,1)
plt.plot(history_2.history['accuracy'],label='accuarcy')
plt.plot(history_2.history['val_accuracy'],label='val_accuracy')
plt.legend()

plt.subplot(1,2,2)
plt.plot(history_2.history['loss'],label='loss')
plt.plot(history_2.history['val_loss'],label='val_loss')
plt.legend()

plt.show()

"""## **RESENETV2**"""

# Creating a callback function to save model's metrics
def create_tensorboard_callback(dir_name, experiment_name):
    log_dir= dir_name + "/" + experiment_name + "/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
    tensorboard_callback = tf.keras.callbacks.TensorBoard(
      log_dir=log_dir
     )
    print(f"Saving TensorBoard log files to: {log_dir}")
    return tensorboard_callback

# Creating model using Resnet50V2 architecture
tl_model = tf.keras.applications.ResNet50V2(include_top=False)
tl_model.trainable = False
inputs = layers.Input(shape=(200, 200, 3), name='input_layer')
x = tl_model(inputs)
x = layers.GlobalAveragePooling2D()(x)
outputs = layers.Dense(7, activation='softmax')(x)
model_3= tf.keras.Model(inputs, outputs)
model_3.summary()

model_3.compile(optimizer='adam',
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
              metrics=['accuracy'])

epochs=40
history_3 = model_3.fit(
  train_ds,
  validation_data=val_ds,
  epochs=epochs
)

scores=model_3.evaluate(test_ds)

import matplotlib.pyplot as plt

plt.figure(figsize=(8,6))

plt.subplot(1,2,1)
plt.plot(history_3.history['loss'],label='loss')
plt.plot(history_3.history['val_loss'],label='val_loss')
plt.legend()

plt.subplot(1,2,2)
plt.plot(history_3.history['accuracy'],label='accuarcy')
plt.plot(history_3.history['val_accuracy'],label='val_accuracy')

plt.legend()
plt.show()

"""## **InceptionV3**"""

from tensorflow.keras.models import Model
from tensorflow.keras.losses import SparseCategoricalCrossentropy

input_shape = (200, 200, 3)
inputs = Input(shape=input_shape)

# Stem
x = Conv2D(filters=32, kernel_size=(3, 3), strides=(2, 2), padding='valid', activation='relu')(inputs)
x = Conv2D(filters=32, kernel_size=(3, 3), padding='valid', activation='relu')(x)
x = Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu')(x)
x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)

x = Conv2D(filters=80, kernel_size=(1, 1), padding='valid', activation='relu')(x)
x = Conv2D(filters=192, kernel_size=(3, 3), padding='valid', activation='relu')(x)
x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)

# Define Inception modules (for simplicity, we'll use a few inceptions)
def inception_module(x, filters_1x1, filters_3x3_reduce, filters_3x3, filters_5x5_reduce, filters_5x5, filters_pool_proj):
    # 1x1 convolution
    conv1x1 = Conv2D(filters_1x1, kernel_size=(1, 1), activation='relu')(x)

    # 3x3 convolution
    conv3x3_reduce = Conv2D(filters_3x3_reduce, kernel_size=(1, 1), activation='relu')(x)
    conv3x3 = Conv2D(filters_3x3, kernel_size=(3, 3), padding='same', activation='relu')(conv3x3_reduce)

    # 5x5 convolution
    conv5x5_reduce = Conv2D(filters_5x5_reduce, kernel_size=(1, 1), activation='relu')(x)
    conv5x5 = Conv2D(filters_5x5, kernel_size=(5, 5), padding='same', activation='relu')(conv5x5_reduce)

    # MaxPooling2D and 1x1 convolution
    pool = MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')(x)
    pool_proj = Conv2D(filters_pool_proj, kernel_size=(1, 1), activation='relu')(pool)

    # Concatenate the layers
    return concatenate([conv1x1, conv3x3, conv5x5, pool_proj], axis=-1)

# Add Inception modules (for simplicity, we're adding a few)
x = inception_module(x, 64, 48, 64, 64, 96, 32)
x = inception_module(x, 64, 48, 64, 64, 96, 32)
x = inception_module(x, 64, 48, 64, 64, 96, 32)
x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)

# Flatten and add fully connected layers
x = Flatten(name='flatten')(x)
x = Dense(4096, activation='relu')(x)
x = Dropout(0.5)(x)
x = Dense(4096, activation='relu')(x)
x = Dropout(0.5)(x)
x = Dense(128, activation='softmax')(x)

# Create the model
model4 = Model(inputs=inputs, outputs=x)

# Display the model summary
model4.summary()

model4.compile(loss=SparseCategoricalCrossentropy(),optimizer='adam', metrics=['accuracy'])

history_4=model4.fit(train_ds,epochs=50,batch_size=32,validation_data=(val_ds))

scores=model4.evaluate(test_ds)

import matplotlib.pyplot as plt

plt.figure(figsize=(8,6))

plt.subplot(1,2,1)
plt.plot(history_4.history['loss'],label='loss')
plt.plot(history_4.history['val_loss'],label='val_loss')
plt.legend()

plt.subplot(1,2,2)
plt.plot(history_4.history['accuracy'],label='accuarcy')
plt.plot(history_4.history['val_accuracy'],label='val_accuracy')

plt.legend()
plt.show()

"""#Model evaluation

"""

from tensorflow.keras.models import load_model

model_2.save('/content/drive/MyDrive/Model-new/chiili_model.h5')

from tensorflow.keras.models import load_model

model_2.save('/content/drive/MyDrive/Model-new/chiili_model.keras')

history_2.history

#recording history in json
import json
with open("training_hist.json","w") as f:
  json.dump(history_2.history,f)

y_pred_2=model_2.predict(test_ds)
y_pred_labels = np.argmax(y_pred_2, axis=1)

import numpy as np

# Extract true labels (y_true) from val_ds
y_true = []
for images, labels in test_ds:
    # Convert labels tensor to numpy array and append to y_true list
    y_true.append(labels.numpy())

# Convert y_true list into a single numpy array
y_true = np.concatenate(y_true)

# Now y_true contains the true labels from the validation dataset as a numpy array

from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, classification_report

# Calculate accuracy
accuracy = accuracy_score(y_true, y_pred_labels)
print(f"Accuracy: {accuracy:.2f}")

# Calculate F1 score (weighted average)
f1 = f1_score(y_true, y_pred_labels, average='weighted')
print(f"F1 Score: {f1:.2f}")

# Calculate precision
precision = precision_score(y_true, y_pred_labels, average='weighted')
print(f"Precision: {precision:.2f}")

# Calculate recall
recall = recall_score(y_true, y_pred_labels, average='weighted')
print(f"Recall: {recall:.2f}")

# Calculate confusion matrix
conf_matrix = confusion_matrix(y_true, y_pred_labels)
print(f"Confusion Matrix:\n{conf_matrix}")

# Generate classification report
print("\nClassification Report:\n", classification_report(y_true, y_pred_labels))

y_pred=model_2.predict(test_ds)
y_pred,y_pred.shape

predicted_categories=tf.argmax(y_pred,axis=1)

predicted_categories

for x,y in train_ds:
  print(x,x.shape)
  print(y,y.shape)
  break

y_true=tf.concat([y for x,y in val_ds],axis=0)
y_true

from sklearn.metrics import classification_report,confusion_matrix

cm=confusion_matrix(y_true,predicted_categories)
cm

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(6,4))
sns.heatmap(cm,annot=True,annot_kws={'size':8})
plt.xlabel('predicted class',fontsize=10)
plt.ylabel('true class',fontsize=10)
plt.title('confusion matrix',fontsize=15)
plt.show()

import pickle
pickle.dump(model_2,open('/content/drive/MyDrive/Model/chiil_model.pk','wb'))

# Load the Keras model from an HDF5 file
model = keras.models.load_model('/content/drive/MyDrive/Model/chiili_model.h5')

# Create a TFLiteConverter
converter = tf.lite.TFLiteConverter.from_keras_model(model)

converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,  # TensorFlow Lite ops
    tf.lite.OpsSet.SELECT_TF_OPS    # TensorFlow Select ops
]

# Optional: Configure optimizations (e.g., quantization)
# converter.optimizations = [tf.lite.Optimize.DEFAULT]

# Convert the Keras model to TensorFlow Lite format
tflite_model = converter.convert()

# Save the TFLite model to a file
with open('/content/drive/MyDrive/Model/chiili_model.tflite', 'wb') as f:
    f.write(tflite_model)

print("Model converted to TFLite format and saved to file.")

